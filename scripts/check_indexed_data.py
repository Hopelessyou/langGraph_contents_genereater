#!/usr/bin/env python3
"""ì¸ë±ì‹±ëœ ë°ì´í„° í™•ì¸ ìŠ¤í¬ë¦½íŠ¸"""

import sys
from pathlib import Path
import asyncio
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from src.rag.vector_store import VectorStore
from src.rag.embedding import EmbeddingGenerator
from src.rag.retriever import HybridRetriever
from src.rag.indexer import DocumentIndexer
from src.rag.incremental_updater import IncrementalUpdater
from config.settings import settings

logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)


async def check_indexed_data():
    """ì¸ë±ì‹±ëœ ë°ì´í„° ì¢…í•© í™•ì¸"""
    
    print("=" * 60)
    print("ðŸ“Š ì¸ë±ì‹± ë°ì´í„° í™•ì¸")
    print("=" * 60)
    
    # 1. ë²¡í„° DB ìƒíƒœ í™•ì¸
    print("\n1ï¸âƒ£ ë²¡í„° DB ìƒíƒœ")
    print("-" * 60)
    try:
        vector_store = VectorStore(collection_name="legal_documents")
        count = vector_store.get_count()  # ë™ê¸° ë©”ì„œë“œ
        print(f"   âœ… ë²¡í„° DB ì²­í¬ ìˆ˜: {count}ê°œ")
        print(f"   ì»¬ë ‰ì…˜ ì´ë¦„: {vector_store.collection_name}")
        
        if count == 0:
            print("\nâš ï¸  ë²¡í„° DBê°€ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤!")
            print("   ë°ì´í„°ë¥¼ ì¸ë±ì‹±í•˜ì„¸ìš”:")
            print("   python scripts/process_and_index.py --input-dir data/processed/statutes --doc-type statute")
            print("   python scripts/process_and_index.py --input-dir data/processed/cases --doc-type case")
            return False
    except Exception as e:
        print(f"   âŒ ë²¡í„° DB í™•ì¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # 2. ì¸ë±ì‹± ìƒíƒœ í™•ì¸
    print("\n2ï¸âƒ£ ì¸ë±ì‹± ìƒíƒœ")
    print("-" * 60)
    try:
        embedding_gen = EmbeddingGenerator()
        indexer = DocumentIndexer(collection_name="legal_documents")
        updater = IncrementalUpdater(
            indexer=indexer,
            state_file=Path("./data/index_state.json")
        )
        
        status = updater.get_status()
        print(f"   ì¸ë±ì‹±ëœ ë¬¸ì„œ ìˆ˜: {status['indexed_count']}ê°œ")
        print(f"   ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {status.get('last_updated', 'N/A')}")
    except Exception as e:
        print(f"   âš ï¸  ì¸ë±ì‹± ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        # ìƒíƒœ íŒŒì¼ì´ ì—†ì–´ë„ ê³„ì† ì§„í–‰
    
    # 3. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n3ï¸âƒ£ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("-" * 60)
    try:
        retriever = HybridRetriever(vector_store, embedding_gen)
        test_query = "ì‚¬ê¸°"
        
        print(f"   í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{test_query}'")
        search_results = await retriever.search(
            query=test_query,
            n_results=5
        )
        
        if search_results.get('results') and len(search_results['results']) > 0:
            print(f"   âœ… ê²€ìƒ‰ ì„±ê³µ: {len(search_results['results'])}ê°œ ê²°ê³¼")
            print("\n   ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ):")
            for i, result in enumerate(search_results['results'][:3], 1):
                print(f"   {i}. ID: {result.get('id', 'N/A')}")
                metadata = result.get('metadata', {})
                print(f"      ì œëª©: {metadata.get('title', 'N/A')}")
                print(f"      íƒ€ìž…: {metadata.get('type', 'N/A')}")
                print(f"      ê±°ë¦¬: {result.get('distance', 'N/A'):.4f}")
                doc_preview = result.get('document', '')[:50]
                print(f"      ë‚´ìš©: {doc_preview}...")
        else:
            print(f"   âŒ ê²€ìƒ‰ ì‹¤íŒ¨: ê²°ê³¼ ì—†ìŒ")
            print("   âš ï¸  ë°ì´í„°ê°€ ì¸ë±ì‹±ë˜ì§€ ì•Šì•˜ê±°ë‚˜, ê²€ìƒ‰ ì¿¼ë¦¬ì™€ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"   âš ï¸  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    
    # 4. ë¬¸ì„œ íƒ€ìž…ë³„ í†µê³„
    print("\n4ï¸âƒ£ ë¬¸ì„œ íƒ€ìž…ë³„ í†µê³„")
    print("-" * 60)
    try:
        import chromadb
        from chromadb.config import Settings
        
        client = chromadb.PersistentClient(
            path=str(Path("./data/vector_db")),
            settings=Settings(anonymized_telemetry=False)
        )
        collection = client.get_collection(vector_store.collection_name)
        
        # ìƒ˜í”Œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë©”íƒ€ë°ì´í„°ë§Œ)
        sample_data = collection.get(limit=1000)  # ë” ë§Žì€ ìƒ˜í”Œ
        
        if sample_data.get('metadatas'):
            type_counts = {}
            for metadata in sample_data['metadatas']:
                doc_type = metadata.get('type', 'unknown')
                type_counts[doc_type] = type_counts.get(doc_type, 0) + 1
            
            if type_counts:
                for doc_type, count in sorted(type_counts.items()):
                    print(f"   {doc_type}: {count}ê°œ ì²­í¬ (ìƒ˜í”Œ)")
            else:
                print("   í†µê³„ ìˆ˜ì§‘ ë¶ˆê°€")
        else:
            print("   ë©”íƒ€ë°ì´í„° ì—†ìŒ")
    except Exception as e:
        print(f"   âš ï¸  í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 60)
    if count > 0:
        print("âœ… ë°ì´í„°ê°€ ì¸ë±ì‹±ë˜ì–´ ìžˆìŠµë‹ˆë‹¤!")
    else:
        print("âŒ ë°ì´í„°ê°€ ì¸ë±ì‹±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("=" * 60)
    
    return count > 0


if __name__ == "__main__":
    asyncio.run(check_indexed_data())

