# ë°ì´í„° ì „ì²˜ë¦¬ ì½”ë“œ ì˜ˆì‹œ ğŸ“

ì‹¤ì œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ì „ì²˜ë¦¬ ì½”ë“œ ì˜ˆì‹œì…ë‹ˆë‹¤.

---

## ëª©ì°¨

1. [ê°€ì¥ ì‰¬ìš´ ë°©ë²•: ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©](#1-ê°€ì¥-ì‰¬ìš´-ë°©ë²•-ìŠ¤í¬ë¦½íŠ¸-ì‚¬ìš©)
2. [Python ì½”ë“œë¡œ ì§ì ‘ ì‹¤í–‰](#2-python-ì½”ë“œë¡œ-ì§ì ‘-ì‹¤í–‰)
3. [ë‹¨ê³„ë³„ë¡œ ì§ì ‘ ì‹¤í–‰](#3-ë‹¨ê³„ë³„ë¡œ-ì§ì ‘-ì‹¤í–‰)
4. [ì‹¤ì „ ì˜ˆì œ](#4-ì‹¤ì „-ì˜ˆì œ)

---

## 1. ê°€ì¥ ì‰¬ìš´ ë°©ë²•: ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©

### 1.1 ê¸°ë³¸ ì‚¬ìš©ë²•

```bash
# ë²•ë ¹ ë°ì´í„° ì „ì²˜ë¦¬
python scripts/process_and_index.py \
    --input-dir "data/collected/statutes" \
    --doc-type "statute"

# íŒë¡€ ë°ì´í„° ì „ì²˜ë¦¬
python scripts/process_and_index.py \
    --input-dir "data/collected/cases" \
    --doc-type "case"
```

### 1.2 ì˜µì…˜ ì„¤ëª…

```bash
# ì „ì²´ ì˜µì…˜ ë³´ê¸°
python scripts/process_and_index.py --help

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ì§€ì •
python scripts/process_and_index.py \
    --input-dir "data/collected/statutes" \
    --output-dir "data/processed/statutes" \
    --doc-type "statute"

# ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰ (ì¸ë±ì‹± ê±´ë„ˆë›°ê¸°)
python scripts/process_and_index.py \
    --input-dir "data/collected/statutes" \
    --doc-type "statute" \
    --skip-index

# ì¸ë±ì‹±ë§Œ ìˆ˜í–‰ (ì „ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°)
python scripts/process_and_index.py \
    --input-dir "data/processed/statutes" \
    --doc-type "statute" \
    --skip-process
```

### 1.3 Windows PowerShell ì‚¬ìš©ë²•

```powershell
# PowerShellì—ì„œ ì‹¤í–‰
python scripts/process_and_index.py `
    --input-dir "data/collected/statutes" `
    --doc-type "statute"
```

---

## 2. Python ì½”ë“œë¡œ ì§ì ‘ ì‹¤í–‰

### 2.1 ê¸°ë³¸ ì˜ˆì œ: ë””ë ‰í† ë¦¬ ì „ì²´ ì²˜ë¦¬

```python
#!/usr/bin/env python3
"""ë°ì´í„° ì „ì²˜ë¦¬ ê¸°ë³¸ ì˜ˆì œ"""

from src.processors.pipeline import BatchProcessor
from pathlib import Path

# ì „ì²˜ë¦¬ê¸° ìƒì„±
processor = BatchProcessor()

# ì „ì²˜ë¦¬ ì‹¤í–‰
results = processor.process_directory(
    input_dir="data/collected/statutes",      # ì›ë³¸ ë°ì´í„° ìœ„ì¹˜
    output_dir="data/processed/statutes",     # ì €ì¥í•  ìœ„ì¹˜
    doc_type="statute",                       # ë¬¸ì„œ íƒ€ì…
    clean=True,                               # ì •ì œ ìˆ˜í–‰
    validate=True,                            # ê²€ì¦ ìˆ˜í–‰
    remove_duplicates=True,                   # ì¤‘ë³µ ì œê±°
)

# ê²°ê³¼ í™•ì¸
total = len(results)
success = sum(1 for success, _ in results.values() if success)
failed = total - success

print(f"\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
print(f"   ì´ íŒŒì¼: {total}ê°œ")
print(f"   âœ… ì„±ê³µ: {success}ê°œ")
print(f"   âŒ ì‹¤íŒ¨: {failed}ê°œ")

# ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡
if failed > 0:
    print(f"\nâš ï¸  ì‹¤íŒ¨í•œ íŒŒì¼:")
    for filename, (success, error) in results.items():
        if not success:
            print(f"   - {filename}: {error}")
```

### 2.2 ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬

```python
from src.processors.pipeline import BatchProcessor
from pathlib import Path

processor = BatchProcessor()

# ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
success, error = processor.process_file(
    input_path="data/collected/statutes/í˜•ë²•/statute-í˜•ë²•-347.json",
    output_path="data/processed/statutes/statute-í˜•ë²•-347.json",
    doc_type="statute",
    clean=True,
    validate=True,
)

if success:
    print("âœ… íŒŒì¼ ì²˜ë¦¬ ì„±ê³µ!")
else:
    print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {error}")
```

### 2.3 ì—¬ëŸ¬ ë¬¸ì„œ íƒ€ì… ì¼ê´„ ì²˜ë¦¬

```python
from src.processors.pipeline import BatchProcessor

processor = BatchProcessor()

# ì²˜ë¦¬í•  ë¬¸ì„œ íƒ€ì… ëª©ë¡
doc_types = {
    "statute": "data/collected/statutes",
    "case": "data/collected/cases",
    "procedure": "data/collected/procedures",
}

# ê° íƒ€ì…ë³„ë¡œ ì²˜ë¦¬
for doc_type, input_dir in doc_types.items():
    print(f"\n{'='*60}")
    print(f"ì²˜ë¦¬ ì¤‘: {doc_type}")
    print(f"{'='*60}")
    
    results = processor.process_directory(
        input_dir=input_dir,
        output_dir=f"data/processed/{doc_type}",
        doc_type=doc_type,
        clean=True,
        validate=True,
        remove_duplicates=True,
    )
    
    # í†µê³„ ì¶œë ¥
    total = len(results)
    success = sum(1 for s, _ in results.values() if s)
    print(f"âœ… {doc_type}: {success}/{total} ì„±ê³µ")
```

---

## 3. ë‹¨ê³„ë³„ë¡œ ì§ì ‘ ì‹¤í–‰

### 3.1 ì „ì²´ ë‹¨ê³„ í¬í•¨ ì˜ˆì œ

```python
#!/usr/bin/env python3
"""ë‹¨ê³„ë³„ ë°ì´í„° ì „ì²˜ë¦¬ ì˜ˆì œ"""

from src.processors.validator import DocumentValidator
from src.processors.cleaner import DataCleaner
from src.processors.converter import JSONConverter
from pathlib import Path
import json

def preprocess_single_file(input_file: Path, output_file: Path, doc_type: str):
    """ë‹¨ì¼ íŒŒì¼ì„ ë‹¨ê³„ë³„ë¡œ ì „ì²˜ë¦¬"""
    
    print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {input_file.name}")
    
    # 1. ì›ë³¸ ë°ì´í„° ì½ê¸°
    try:
        with open(input_file, "r", encoding="utf-8") as f:
            raw_data = json.load(f)
        print("   âœ… íŒŒì¼ ì½ê¸° ì™„ë£Œ")
    except Exception as e:
        print(f"   âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return False
    
    # 2. í˜•ì‹ ë³€í™˜
    converter = JSONConverter()
    standard_data = converter.convert_to_standard_format(raw_data, doc_type)
    
    if not standard_data:
        print("   âŒ í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨")
        return False
    print("   âœ… í˜•ì‹ ë³€í™˜ ì™„ë£Œ")
    
    # 3. ë°ì´í„° ì •ì œ
    cleaner = DataCleaner()
    cleaned_data = cleaner.clean(standard_data)
    print("   âœ… ë°ì´í„° ì •ì œ ì™„ë£Œ")
    
    # 4. í•„ìˆ˜ í•„ë“œ ê²€ì¦
    valid, errors = cleaner.validate_required_fields(cleaned_data)
    if not valid:
        print(f"   âŒ í•„ìˆ˜ í•„ë“œ ê²€ì¦ ì‹¤íŒ¨: {', '.join(errors)}")
        return False
    print("   âœ… í•„ìˆ˜ í•„ë“œ ê²€ì¦ ì™„ë£Œ")
    
    # 5. ìµœì¢… ê²€ì¦
    validator = DocumentValidator()
    success, model = validator.validate(cleaned_data)
    
    if not success:
        print(f"   âŒ ìµœì¢… ê²€ì¦ ì‹¤íŒ¨:")
        for error in validator.get_errors():
            print(f"      - {error}")
        return False
    print("   âœ… ìµœì¢… ê²€ì¦ ì™„ë£Œ")
    
    # 6. ì €ì¥
    try:
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(model.model_dump(), f, ensure_ascii=False, indent=2)
        print(f"   âœ… ì €ì¥ ì™„ë£Œ: {output_file.name}")
        return True
    except Exception as e:
        print(f"   âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    input_file = Path("data/collected/statutes/í˜•ë²•/statute-í˜•ë²•-347.json")
    output_file = Path("data/processed/statutes/statute-í˜•ë²•-347.json")
    
    success = preprocess_single_file(input_file, output_file, "statute")
    
    if success:
        print("\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
    else:
        print("\nâŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨!")
```

### 3.2 ê° ë‹¨ê³„ë³„ ìƒì„¸ ì˜ˆì œ

```python
from src.processors.validator import DocumentValidator
from src.processors.cleaner import DataCleaner
from src.processors.converter import JSONConverter
import json

# ì›ë³¸ ë°ì´í„°
raw_data = {
    "ë²•ë¥ ëª…": "í˜•ë²•",
    "ì¡°ë¬¸ë²ˆí˜¸": "347",
    "ì œëª©": "  í˜•ë²• ì œ347ì¡°  ",
    "ë‚´ìš©": "ì‚¬ê¸°ì£„ì˜   ë‚´ìš©ì…ë‹ˆë‹¤.\n\n\nì—¬ëŸ¬ ì¤„ë°”ê¿ˆ",
}

# 1ë‹¨ê³„: í˜•ì‹ ë³€í™˜
converter = JSONConverter()
standard_data = converter.convert_to_standard_format(raw_data, "statute")
print("1ë‹¨ê³„ ì™„ë£Œ: í˜•ì‹ ë³€í™˜")
print(json.dumps(standard_data, ensure_ascii=False, indent=2))

# 2ë‹¨ê³„: ë°ì´í„° ì •ì œ
cleaner = DataCleaner()
cleaned_data = cleaner.clean(standard_data)
print("\n2ë‹¨ê³„ ì™„ë£Œ: ë°ì´í„° ì •ì œ")
print(json.dumps(cleaned_data, ensure_ascii=False, indent=2))

# 3ë‹¨ê³„: ê²€ì¦
validator = DocumentValidator()
success, model = validator.validate(cleaned_data)

if success:
    print("\n3ë‹¨ê³„ ì™„ë£Œ: ê²€ì¦ ì„±ê³µ")
    print(f"ë¬¸ì„œ ID: {model.id}")
    print(f"ì œëª©: {model.title}")
else:
    print("\n3ë‹¨ê³„ ì‹¤íŒ¨: ê²€ì¦ ì‹¤íŒ¨")
    for error in validator.get_errors():
        print(f"  - {error}")
```

---

## 4. ì‹¤ì „ ì˜ˆì œ

### 4.1 ì™„ì „í•œ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸

```python
#!/usr/bin/env python3
"""ì™„ì „í•œ ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from src.processors.pipeline import BatchProcessor
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s'
)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    processor = BatchProcessor()
    
    # ì²˜ë¦¬í•  ë°ì´í„° ëª©ë¡
    data_configs = [
        {
            "name": "ë²•ë ¹",
            "input_dir": "data/collected/statutes",
            "output_dir": "data/processed/statutes",
            "doc_type": "statute",
        },
        {
            "name": "íŒë¡€",
            "input_dir": "data/collected/cases",
            "output_dir": "data/processed/cases",
            "doc_type": "case",
        },
    ]
    
    # ì „ì²´ í†µê³„
    total_files = 0
    total_success = 0
    total_failed = 0
    
    # ê° ë°ì´í„° íƒ€ì…ë³„ë¡œ ì²˜ë¦¬
    for config in data_configs:
        print(f"\n{'='*60}")
        print(f"ğŸ“š {config['name']} ë°ì´í„° ì „ì²˜ë¦¬")
        print(f"{'='*60}")
        
        input_dir = Path(config['input_dir'])
        if not input_dir.exists():
            print(f"âš ï¸  ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
            continue
        
        # ì „ì²˜ë¦¬ ì‹¤í–‰
        results = processor.process_directory(
            input_dir=config['input_dir'],
            output_dir=config['output_dir'],
            doc_type=config['doc_type'],
            clean=True,
            validate=True,
            remove_duplicates=True,
        )
        
        # í†µê³„ ê³„ì‚°
        total = len(results)
        success = sum(1 for s, _ in results.values() if s)
        failed = total - success
        
        total_files += total
        total_success += success
        total_failed += failed
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š {config['name']} ì²˜ë¦¬ ê²°ê³¼:")
        print(f"   ì´ íŒŒì¼: {total}ê°œ")
        print(f"   âœ… ì„±ê³µ: {success}ê°œ")
        print(f"   âŒ ì‹¤íŒ¨: {failed}ê°œ")
        
        # ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡
        if failed > 0:
            print(f"\nâš ï¸  ì‹¤íŒ¨í•œ íŒŒì¼:")
            for filename, (s, error) in results.items():
                if not s:
                    print(f"   - {filename}: {error}")
    
    # ì „ì²´ í†µê³„ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ì „ì²´ ì²˜ë¦¬ ê²°ê³¼")
    print(f"{'='*60}")
    print(f"   ì´ íŒŒì¼: {total_files}ê°œ")
    print(f"   âœ… ì„±ê³µ: {total_success}ê°œ")
    print(f"   âŒ ì‹¤íŒ¨: {total_failed}ê°œ")
    print(f"   ì„±ê³µë¥ : {total_success/total_files*100:.1f}%" if total_files > 0 else "   ì„±ê³µë¥ : 0%")

if __name__ == "__main__":
    main()
```

### 4.2 ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨ ì˜ˆì œ

```python
from src.processors.pipeline import BatchProcessor
from pathlib import Path
import json

def preprocess_with_error_handling(input_dir: str, output_dir: str, doc_type: str):
    """ì—ëŸ¬ ì²˜ë¦¬ë¥¼ í¬í•¨í•œ ì „ì²˜ë¦¬"""
    
    processor = BatchProcessor()
    
    try:
        # ì „ì²˜ë¦¬ ì‹¤í–‰
        results = processor.process_directory(
            input_dir=input_dir,
            output_dir=output_dir,
            doc_type=doc_type,
            clean=True,
            validate=True,
            remove_duplicates=True,
        )
        
        # ì„±ê³µ/ì‹¤íŒ¨ ë¶„ë¥˜
        success_files = []
        failed_files = []
        
        for filename, (success, error) in results.items():
            if success:
                success_files.append(filename)
            else:
                failed_files.append((filename, error))
        
        # ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            "total": len(results),
            "success": len(success_files),
            "failed": len(failed_files),
            "success_files": success_files,
            "failed_files": [
                {"filename": f, "error": e} for f, e in failed_files
            ],
        }
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_file = Path(output_dir) / "preprocessing_report.json"
        report_file.parent.mkdir(parents=True, exist_ok=True)
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ì„±ê³µ: {report['success']}/{report['total']}")
        print(f"   ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
        
        return report
        
    except Exception as e:
        print(f"âŒ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    report = preprocess_with_error_handling(
        input_dir="data/collected/statutes",
        output_dir="data/processed/statutes",
        doc_type="statute",
    )
```

### 4.3 ì§„í–‰ ìƒí™© í‘œì‹œ ì˜ˆì œ

```python
from src.processors.pipeline import BatchProcessor
from pathlib import Path
from tqdm import tqdm  # pip install tqdm í•„ìš”

def preprocess_with_progress(input_dir: str, output_dir: str, doc_type: str):
    """ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•˜ëŠ” ì „ì²˜ë¦¬"""
    
    processor = BatchProcessor()
    input_path = Path(input_dir)
    
    # ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡
    json_files = list(input_path.rglob("*.json"))
    
    if not json_files:
        print("âš ï¸  ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ì´ {len(json_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...\n")
    
    results = {}
    
    # ì§„í–‰ ìƒí™© í‘œì‹œ
    for json_file in tqdm(json_files, desc="ì „ì²˜ë¦¬ ì§„í–‰"):
        relative_path = json_file.relative_to(input_path)
        output_file = Path(output_dir) / relative_path
        
        success, error = processor.process_file(
            input_path=json_file,
            output_path=output_file,
            doc_type=doc_type,
            clean=True,
            validate=True,
        )
        
        results[json_file.name] = (success, error)
    
    # ê²°ê³¼ ì¶œë ¥
    total = len(results)
    success = sum(1 for s, _ in results.values() if s)
    failed = total - success
    
    print(f"\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
    print(f"   âœ… ì„±ê³µ: {success}ê°œ")
    print(f"   âŒ ì‹¤íŒ¨: {failed}ê°œ")
    
    return results


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    preprocess_with_progress(
        input_dir="data/collected/statutes",
        output_dir="data/processed/statutes",
        doc_type="statute",
    )
```

### 4.4 ì¬ê·€ì ìœ¼ë¡œ í•˜ìœ„ ë””ë ‰í† ë¦¬ ì²˜ë¦¬

```python
from src.processors.pipeline import BatchProcessor
from pathlib import Path

def preprocess_recursive(input_dir: str, output_dir: str, doc_type: str):
    """í•˜ìœ„ ë””ë ‰í† ë¦¬ë¥¼ í¬í•¨í•˜ì—¬ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬"""
    
    processor = BatchProcessor()
    input_path = Path(input_dir)
    
    # ëª¨ë“  JSON íŒŒì¼ ì°¾ê¸° (í•˜ìœ„ ë””ë ‰í† ë¦¬ í¬í•¨)
    json_files = list(input_path.rglob("*.json"))
    
    print(f"ğŸ“ ì´ {len(json_files)}ê°œ íŒŒì¼ ë°œê²¬\n")
    
    results = {}
    
    for json_file in json_files:
        # ìƒëŒ€ ê²½ë¡œ ìœ ì§€
        relative_path = json_file.relative_to(input_path)
        output_file = Path(output_dir) / relative_path
        
        print(f"ì²˜ë¦¬ ì¤‘: {relative_path}")
        
        success, error = processor.process_file(
            input_path=json_file,
            output_path=output_file,
            doc_type=doc_type,
            clean=True,
            validate=True,
        )
        
        results[str(relative_path)] = (success, error)
        
        if success:
            print(f"  âœ… ì„±ê³µ\n")
        else:
            print(f"  âŒ ì‹¤íŒ¨: {error}\n")
    
    return results


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # í•˜ìœ„ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìœ ì§€í•˜ë©´ì„œ ì²˜ë¦¬
    # data/collected/statutes/í˜•ë²•/statute-347.json
    # â†’ data/processed/statutes/í˜•ë²•/statute-347.json
    preprocess_recursive(
        input_dir="data/collected/statutes",
        output_dir="data/processed/statutes",
        doc_type="statute",
    )
```

---

## ë¹ ë¥¸ ì°¸ì¡°

### ê°€ì¥ ê°„ë‹¨í•œ ì‚¬ìš©ë²•

```python
from src.processors.pipeline import BatchProcessor

processor = BatchProcessor()
results = processor.process_directory(
    input_dir="data/collected/statutes",
    output_dir="data/processed/statutes",
    doc_type="statute",
)
```

### ëª…ë ¹ì¤„ì—ì„œ ì‹¤í–‰

```bash
python scripts/process_and_index.py \
    --input-dir "data/collected/statutes" \
    --doc-type "statute"
```

---

## ë¬¸ì œ í•´ê²°

### Q: "ModuleNotFoundError: No module named 'src'" ì—ëŸ¬

**í•´ê²° ë°©ë²•:**
```python
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
```

### Q: "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤" ì—ëŸ¬

**í•´ê²° ë°©ë²•:**
```python
from pathlib import Path

input_dir = Path("data/collected/statutes")
if not input_dir.exists():
    print(f"âš ï¸  ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
    print("   ë¨¼ì € ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.")
```

### Q: "ê²€ì¦ ì‹¤íŒ¨" ì—ëŸ¬

**í•´ê²° ë°©ë²•:**
```python
from src.processors.validator import DocumentValidator

validator = DocumentValidator()
success, model = validator.validate(data)

if not success:
    # ì—ëŸ¬ í™•ì¸
    for error in validator.get_errors():
        print(f"  - {error}")
    
    # ë°ì´í„° ìˆ˜ì • í›„ ë‹¤ì‹œ ì‹œë„
```

---

**ë” ìì„¸í•œ ë‚´ìš©ì€ ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”:**
- [ë°ì´í„° ì „ì²˜ë¦¬ ì‰½ê²Œ ì´í•´í•˜ê¸°](./DATA_PREPROCESSING_EASY_GUIDE.md)
- [DocumentValidator ì‚¬ìš© ê°€ì´ë“œ](./DOCUMENT_VALIDATOR_GUIDE.md)
- [RAG ë°ì´í„° ì²˜ë¦¬ ê°€ì´ë“œ](./RAG_DATA_PROCESSING_GUIDE.md)

